<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>nn-UNet &mdash; cvpl_tools 0.8.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=ce6c0465"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Beta-Amyloid Plaque Detection" href="beta_amyloid_plaque_detection.html" />
    <link rel="prev" title="Result Caching" href="result_caching.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            cvpl_tools
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="ome_zarr.html">Viewing and IO of OME Zarr</a></li>
<li class="toctree-l1"><a class="reference internal" href="setting_up_the_script.html">Setting Up the Script</a></li>
<li class="toctree-l1"><a class="reference internal" href="segmentation_pipeline.html">Defining Segmentation Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="result_caching.html">Result Caching</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">nn-UNet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#negative-masking-for-mouse-brain-lightsheet">Negative Masking for Mouse-brain Lightsheet</a></li>
<li class="toctree-l2"><a class="reference internal" href="#annotation">Annotation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prediction">Prediction</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="beta_amyloid_plaque_detection.html">Beta-Amyloid Plaque Detection</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../API/napari_zarr.html">cvpl_tools.ome_zarr.napari.add.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../API/ome_zarr_io.html">cvpl_tools.ome_zarr.io.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../API/tlfs.html">cvpl_tools.tools.fs.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../API/ndblock.html">cvpl_tools.im.ndblock.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../API/seg_process.html">cvpl_tools.im.process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../API/algs.html">cvpl_tools.im.algs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">cvpl_tools</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">nn-UNet</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/GettingStarted/nnunet.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nn-unet">
<span id="nnunet"></span><h1>nn-UNet<a class="headerlink" href="#nn-unet" title="Permalink to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>nn-UNet is a 2d/3d U-NET library designed to segment medical images, refer to
<a class="reference external" href="https://github.com/MIC-DKFZ/nnUNet">github</a> and the following citation:</p>
<ul class="simple">
<li><p>Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., &amp; Maier-Hein, K. H. (2021). nnU-Net: a self-configuring
method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.</p></li>
</ul>
<p>nn-UNet is easiest to use with their command line interface with three commands <code class="code docutils literal notranslate"><span class="pre">nnUNetv2_plan_and_preprocess</span></code>,
<code class="code docutils literal notranslate"><span class="pre">nnUNetv2_train</span></code> and <code class="code docutils literal notranslate"><span class="pre">nnUNetv2_predict</span></code>.</p>
<p>For <code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code>, <code class="code docutils literal notranslate"><span class="pre">cvpl_tools/nnunet/cli.py</span></code> provides two
wrapper command line interface commands <code class="code docutils literal notranslate"><span class="pre">train</span></code> and <code class="code docutils literal notranslate"><span class="pre">predict</span></code> that simplify the three commands into
two and hides unused parameters for SPIMquant workflow.</p>
<p><code class="code docutils literal notranslate"><span class="pre">cvpl_tools/nnunet</span></code> needs torch library and <code class="code docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">nnunetv2</span></code>. GPU is automatically used when
<code class="code docutils literal notranslate"><span class="pre">nnUNetv2_train</span></code> and <code class="code docutils literal notranslate"><span class="pre">nnUNetv2_predict</span></code> are called directly or indirectly through <code class="code docutils literal notranslate"><span class="pre">train</span></code> and
<code class="code docutils literal notranslate"><span class="pre">predict</span></code> and when you have a GPU available on the computer.</p>
<p>For those unfamiliar, nn-UNet has the following quirks:</p>
<ul class="simple">
<li><p>Residual encoder is available for nnunetv2 but we prefer without it since it costs more to train</p></li>
<li><p>Due to limited training data, 2d instead of 3d_fullres mode is used in <code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code></p></li>
<li><p>It trains on images pairs of input size (C, Y, X) and output size (Y, X) where C is number of color channels
(1 in our case), and Y, X are spatial coordinates; specifically, N pairs of images will be provided as training
set and a 80%-20% split will be done for train-validation split which is automatically done by nnUNet. It should
be noted in our case we draw Z images from a single scan volume (C, Z, Y, X), so a random split will have
training set distribution correlated with validation set generated by nnUNet, but such thing is hard to avoid</p></li>
<li><p>The algorithm is not scale-invariant, meaning during prediction, if we zoom the input image by a factor of 2x or
0.5x we get much worse output results. For best results, use the same input/output image sizes as the training
phase. In our mousebrain lightsheet dataset, we downsample the original &gt;200GB dataset by a factor of (4, 8, 8)
before running the nnUNet for training or prediction.</p></li>
<li><p>The algorithm supports the following epochs, useful for small-scale training in our case:
<a class="reference external" href="https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunetv2/training/nnUNetTrainer/variants/training_length/nnUNetTrainer_Xepochs.py">link</a>
if you input number of epochs not listed in this page to the <code class="code docutils literal notranslate"><span class="pre">predict</span></code> command, an error will occur</p></li>
<li><p>nn-UNet supports 5-fold ensemble, which is to run <code class="code docutils literal notranslate"><span class="pre">nnUNetv2_train</span></code> command 5 times each on a different
80%-20% split to obtain 5 models to ensemble the prediction. This does not require rerun <code class="code docutils literal notranslate"><span class="pre">nnUNetv2_plan_and_preprocess</span></code>
and is supported by the <code class="code docutils literal notranslate"><span class="pre">--fold</span></code> argument of <code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code>’ <code class="code docutils literal notranslate"><span class="pre">train</span></code> command so
you don’t need to run it 5 times. If you finish training all folds, you may use the <code class="code docutils literal notranslate"><span class="pre">--fold</span></code> argument of
<code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code>’ <code class="code docutils literal notranslate"><span class="pre">predict</span></code> command to specify <code class="code docutils literal notranslate"><span class="pre">all</span></code> for better accuracy after ensemble or
<code class="code docutils literal notranslate"><span class="pre">0</span></code> to specify using the first fold trained for comparison.</p></li>
<li><p>Running the nn-UNet’s command <code class="code docutils literal notranslate"><span class="pre">nnUNetv2_train</span></code> or <code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code>’ <code class="code docutils literal notranslate"><span class="pre">train</span></code> generates one
<code class="code docutils literal notranslate"><span class="pre">nnUNet_results</span></code> folder, which contains a model (of size a few hundred MBs) and a folder of results
including a loss/DICE graph and a log file containing training losses per epoch and per class. The
same model file is used later for prediction.</p></li>
</ul>
</section>
<section id="negative-masking-for-mouse-brain-lightsheet">
<h2>Negative Masking for Mouse-brain Lightsheet<a class="headerlink" href="#negative-masking-for-mouse-brain-lightsheet" title="Permalink to this heading"></a></h2>
<p>In this section, we focus primarily on the usage of nn-UNet within <code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code>. This part of the
library is designed with handling mouse-brain lightsheet scans in mind. These scans are large (&gt;200GB)
volumes of scans in the format of 4d arrays of data type np.uint16 which is of shape (C, Z, Y, X). An
example is in the google storage bucket
“gcs://khanlab-lightsheet/data/mouse_appmaptapoe/bids/sub-F4A1Te3/micr/sub-F4A1Te3_sample-brain_acq-blaze4x_SPIM.ome.zarr”
with an image shape of (3, 1610, 9653, 9634).</p>
<p>The objective of our algorithm is to quantify the locations and sizes of beta-amyloid plaques in a volume
of lightsheet scan like the above, which appear as small-sized round-shaped bright spots in the image
volume, and can be detected using a simple thresholding method.</p>
<p>Problem comes, however, since the scanned mouse brain edges areas are as bright as the plaques, they
will be marked as false positives. These edges are relatively easier to detect by a UNet algorithm, which
results in the following segmentation workflow we use:</p>
<ol class="arabic simple">
<li><p>For N mousebrain scans M1, …, MN we have at hand, apply bias correction to smooth out within image brightness
difference caused by imaging artifacts</p></li>
<li><p>Then select one of N scans, say M1</p></li>
</ol>
<ol class="arabic simple" start="2">
<li><p>Downsample M1 and use a GUI to paint a binary mask, which contains 1 on regions of edges and 0 on plaques and
elsewhere</p></li>
<li><p>Split the M1 volume and its binary mask annotation vertically to Z slices, and train an nnUNet model on these slices</p></li>
<li><p>Above produces a model that can predict negative masks on any mousebrain scans of the same format; for the rest N-1
mouse brains, they are down-sampled and we use this model to predict on them to obtain their corresponding negative
masks</p></li>
<li><p>These masks are used to remove edge areas of the image before we apply thresholding to find plaque objects.
Algorithmically, we compute M’ where <code class="code docutils literal notranslate"><span class="pre">M'[z,</span> <span class="pre">y,</span> <span class="pre">x]</span> <span class="pre">=</span> <span class="pre">M[z,</span> <span class="pre">y,</span> <span class="pre">x]</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">NEG_MASK[z,</span> <span class="pre">y,</span> <span class="pre">x]</span></code>) for each
voxel location (z, y, x); then, we apply threshold on M’ and take connected component of value of 1 as individual
plaque objects; their centroid locations and sizes (in number of voxels) are summarized in a numpy table and
reported</p></li>
</ol>
<p>In this next part, we discuss the annotation (part 2), training (part 3) and prediction (part 4).</p>
</section>
<section id="annotation">
<h2>Annotation<a class="headerlink" href="#annotation" title="Permalink to this heading"></a></h2>
<p>Data quality is the most crucial to accurate predictions when training supervised models, in which case this is
relevant to us in terms of how well we can annotate 3d image volumes at hand.
Our annotation is the negative masking of edge areas of the
brain to remove edges before applying simple thresholding. We model how good an annotation of negative mask by
looking at:</p>
<ol class="arabic simple">
<li><p>For the simple threshold of choice t, how many voxels are above the threshold across the entire image,
say V</p></li>
<li><p>The number of voxels covered by plaques areas above threshold t, and how many of them are correctly annotated
as 0, and how many of them are incorrectly annotated as 1</p></li>
<li><p>The number of voxels covered by brain edge areas above threshold t, and how many of them are correctly annotated
as 1, and how many of them are incorrectly annotated as 0</p></li>
</ol>
<p>these metrics are best summarized as IOU or DICE scores. A DICE score curve can be obtained in training process,
automatically generated by nn-UNet. We look at an example segmentation below.</p>
<figure class="align-default" id="id1">
<img alt="Slice of mouse brain, unsegmented" src="../_images/mb_unmasked.png" />
<figcaption>
<p><span class="caption-text">Slice of the mouse brain, not annotated (without negative masking)</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<figure class="align-default" id="id2">
<img alt="Slice of mouse brain, negative masked" src="../_images/mb_masked.png" />
<figcaption>
<p><span class="caption-text">Slice of the mouse brain, annotated (with negative masking)</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Here the algorithm, as intended, marks not only the outer edges of the brain but also some of the brighter inner
structures as edge areas to be removed, since they can’t be plaques. The bright spots on the upper left of the images
are left as is, for they are all plaques. Overall, the annotation requires quite a bit of labour and it is preferred
to obtain a high quality annotated volume over many low quality ones.</p>
<p>In <code class="code docutils literal notranslate"><span class="pre">cvpl_tools</span></code>, the annotation is done using a Napari based GUI with a 2d cross-sectional viewer and
ball-shaped paint brush. Follow the following steps to get started:</p>
<ol class="arabic simple">
<li><p>In a Python script, prepare an image you would like to annotate <code class="code docutils literal notranslate"><span class="pre">im_annotate</span></code> in Numpy array format,
which may requires downsample the original image:</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cvpl_tools.nnunet.lightsheet_preprocess</span> <span class="k">as</span> <span class="nn">lightsheet_preprocess</span>

<span class="c1"># original image is, say, an OME ZARR image of size (3, 1610, 9653, 9634)</span>
<span class="n">OME_ZARR_PATH</span> <span class="o">=</span> <span class="s1">&#39;gcs://khanlab-lightsheet/data/mouse_appmaptapoe/bids/sub-F4A1Te3/micr/sub-F4A1Te3_sample-brain_acq-blaze4x_SPIM.ome.zarr&#39;</span>
<span class="n">BA_CHANNEL</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># only the first channel is relevant to Beta-Amyloid detection</span>

<span class="n">FIRST_DOWNSAMPLE_PATH</span> <span class="o">=</span> <span class="s1">&#39;o22/first_downsample.ome.zarr&#39;</span>  <span class="c1"># path to be saved</span>
<span class="n">first_downsample</span> <span class="o">=</span> <span class="n">lightsheet_preprocess</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span>
    <span class="n">OME_ZARR_PATH</span><span class="p">,</span> <span class="n">reduce_fn</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">ndownsample_level</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">ba_channel</span><span class="o">=</span><span class="n">BA_CHANNEL</span><span class="p">,</span>
    <span class="n">write_loc</span><span class="o">=</span><span class="n">FIRST_DOWNSAMPLE_PATH</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape of image after downsampling: </span><span class="si">{</span><span class="n">first_downsample</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Ideally the downsampled image should also go through n4 bias correction before the next step.</p>
<ol class="arabic simple" start="2">
<li><p>Next, convert the image you just downsampled to a numpy array, and use <code class="code docutils literal notranslate"><span class="pre">annotate</span></code> function to add
layers to a napari viewer and start annotation:</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cvpl_tools.nnunet.annotate</span> <span class="kn">import</span> <span class="n">annotate</span>
<span class="kn">import</span> <span class="nn">cvpl_tools.ome_zarr.io</span> <span class="k">as</span> <span class="nn">ome_io</span>
<span class="kn">import</span> <span class="nn">napari</span>

<span class="n">viewer</span> <span class="o">=</span> <span class="n">napari</span><span class="o">.</span><span class="n">Viewer</span><span class="p">(</span><span class="n">ndisplay</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">im_annotate</span> <span class="o">=</span> <span class="n">first_downsample</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>  <span class="c1"># this is a numpy array, to be annotated</span>
<span class="n">ndownsample_level</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># downsample by 2 ^ 1 on three axes</span>

<span class="c1"># image layer and canvas layer will be added here</span>
<span class="n">annotate</span><span class="p">(</span><span class="n">viewer</span><span class="p">,</span> <span class="n">im_annotate</span><span class="p">,</span> <span class="s1">&#39;o22/annotated.tiff&#39;</span><span class="p">,</span> <span class="n">ndownsample_level</span><span class="p">)</span>

<span class="n">viewer</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Note saving is manual, press <code class="code docutils literal notranslate"><span class="pre">ctrl+shift+s</span></code> to save what’s annotated (which creates a tiff
file “o22/annotated.tiff”). <code class="code docutils literal notranslate"><span class="pre">im_annotate</span></code> is lightsheet image first corrected by bias,
then downsampled by levels (1, 2, 2) i.e. a factor of (2, 4, 4) in three directions to a size
that can be conveniently displayed locally, in real-time and without latency.</p>
<p>In this example, we choose to use a binary annotation volume of shape (2, 2, 2) times smaller than the
original image in all three directions. This is to save space during data transfer. Later nn-UNet will
also need image of same shape as the annotation, so we also want to keep a further downsampled image
file that is the same size as the annotation. We will see this in the training section below.</p>
<ol class="arabic simple" start="3">
<li><p>Due to the large image size, you may need multiple sessions in order to completely annotate one
scan. This can be done by running the same code in step 2, which will automatically load the annotation
back up, and you can overwrite the old tiff file with updated annotation by, again, <code class="code docutils literal notranslate"><span class="pre">ctrl+shift+s</span></code></p></li>
</ol>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading"></a></h2>
<p>In the above annotation phase, we obtained two dataset: one is the annotated tiff volume at path
<code class="code docutils literal notranslate"><span class="pre">'o22/annotated.tiff'</span></code>, the other is the downsampled image at path ‘o22/first_downsample.ome.zarr’. We
will use the latter as the training images and the former as the training labels for nn-UNet training.
Here the images need to be once further downsampled in order to match image and label volume shapes:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cvpl_tools.nnunet.lightsheet_preprocess</span> <span class="k">as</span> <span class="nn">lightsheet_preprocess</span>

<span class="n">FIRST_DOWNSAMPLE_PATH</span> <span class="o">=</span> <span class="s1">&#39;o22/first_downsample.ome.zarr&#39;</span>  <span class="c1"># path to be saved</span>
<span class="n">SECOND_DOWNSAMPLE_PATH</span> <span class="o">=</span> <span class="s1">&#39;o22/second_downsample.ome.zarr&#39;</span>
<span class="n">second_downsample</span> <span class="o">=</span> <span class="n">lightsheet_preprocess</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span>
    <span class="n">FIRST_DOWNSAMPLE_PATH</span><span class="p">,</span> <span class="n">reduce_fn</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">,</span> <span class="n">ndownsample_level</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ba_channel</span><span class="o">=</span><span class="n">BA_CHANNEL</span><span class="p">,</span>
    <span class="n">write_loc</span><span class="o">=</span><span class="n">SECOND_DOWNSAMPLE_PATH</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Next, we feed the images to nn-UNet for training. This requires torch installation and a GPU on the
computer.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cvpl_tools.nnunet.triplanar</span> <span class="k">as</span> <span class="nn">triplanar</span>

<span class="n">train_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;cache_url&quot;</span><span class="p">:</span> <span class="s1">&#39;nnunet_trained&#39;</span><span class="p">,</span>  <span class="c1"># this is the path to which training files and trained model will be saved</span>
    <span class="s2">&quot;train_im&quot;</span><span class="p">:</span> <span class="n">SECOND_DOWNSAMPLE_PATH</span><span class="p">,</span>  <span class="c1"># image</span>
    <span class="s2">&quot;train_seg&quot;</span><span class="p">:</span> <span class="s1">&#39;o22/annotated.tiff&#39;</span><span class="p">,</span>  <span class="c1"># label</span>
    <span class="s2">&quot;nepoch&quot;</span><span class="p">:</span> <span class="mi">250</span><span class="p">,</span>
    <span class="s2">&quot;stack_channels&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;triplanar&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;dataset_id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;fold&quot;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span>
    <span class="s2">&quot;max_threshold&quot;</span><span class="p">:</span> <span class="mf">7500.</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">triplanar</span><span class="o">.</span><span class="n">train_triplanar</span><span class="p">(</span><span class="n">train_args</span><span class="p">)</span>
</pre></div>
</div>
<p>250 epochs takes less than half a day to run on a consumer GPU.</p>
</section>
<section id="prediction">
<h2>Prediction<a class="headerlink" href="#prediction" title="Permalink to this heading"></a></h2>
<p>In the training phase we trained our model in the <code class="code docutils literal notranslate"><span class="pre">'nnunet_trained'</span></code> folder. In this folder not everything
is required for prediction, but only the model file in the path
<code class="code docutils literal notranslate"><span class="pre">nnunet_trained/train/yx/nnUNet_results/Dataset001_Lightsheet1/nnUNetTrainer_250epochs__nnUNetPlans__2d/fold_0/checkpoint_final.pth</span></code>
is required. Therefore to reduce file size when you copy this file to other machines for inference, you can
remove the raw and preprocessed folder as well as the <code class="code docutils literal notranslate"><span class="pre">checkpoint_best.pth</span></code> model. Pack the <code class="code docutils literal notranslate"><span class="pre">nnunet_trained</span></code>
folder for prediction, as you will need to specify the this path during prediction.</p>
<p>nn-UNet prediction takes 3 main arguments:</p>
<ol class="arabic simple">
<li><p>Path to your nn-UNet trained folder</p></li>
<li><p>is the tiff file to predict</p></li>
<li><p>output tiff path</p></li>
</ol>
<p>Below gives an example snippet carrying out the prediction on tiff images:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cvpl_tools.nnunet.triplanar</span> <span class="k">as</span> <span class="nn">triplanar</span>

<span class="n">pred_args</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;cache_url&quot;</span><span class="p">:</span> <span class="s1">&#39;nnunet_trained&#39;</span><span class="p">,</span>
    <span class="s2">&quot;test_im&quot;</span><span class="p">:</span> <span class="n">SECOND_DOWNSAMPLE_CORR_PATH</span><span class="p">,</span>
    <span class="s2">&quot;test_seg&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;output&quot;</span><span class="p">:</span> <span class="s1">&#39;output.tiff&#39;</span><span class="p">,</span>
    <span class="s2">&quot;dataset_id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;fold&quot;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span>
    <span class="s2">&quot;triplanar&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;penalize_edge&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;use_cache&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">triplanar</span><span class="o">.</span><span class="n">predict_triplanar</span><span class="p">(</span><span class="n">pred_args</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we are predicting on the training set at SECOND_DOWNSAMPLE_CORR_PATH. In practice we replace this with
other downsampled and corrected mousebrain lightsheet scan volumes. The prediction will automatically use
CPU if GPU is not available; or use GPU if one is. Output tiff can be found at ‘output.tiff’, which should
be the same size as input volume.</p>
<p>Tips on prediction quality:</p>
<p>1. Five fold training or prediction can be specified by setting “fold” to “all”. This will improve accuracy
slightly but takes 5 times the computation resource to train or predict.</p>
<p>2. The tri-planar option will predict the volume in z/y/x three ways and merge the results, which takes 3 times
the computation to train or predict. This significantly increases accuracy, but the result mask is often not
desirable. This is because the ensembed mask often flickers in local areas and can affect contour counting in
our application, and is harder to interpret when looking through yx cross-sectional plane.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="result_caching.html" class="btn btn-neutral float-left" title="Result Caching" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="beta_amyloid_plaque_detection.html" class="btn btn-neutral float-right" title="Beta-Amyloid Plaque Detection" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, KarlHanUW.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>